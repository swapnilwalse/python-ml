{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural networks using perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a perceptron class for logic gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "        Parameters\n",
    "        ------------\n",
    "        eta : Learning rate\n",
    "        n_iter : Number of passes over the training dataset.\n",
    "        Attributes\n",
    "        -----------\n",
    "        w_ : 1d-array, weights after fitting.\n",
    "        errors_ : list, number of misclassifications in every epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta=0.01, n_iter=10, y_values=(0, 1)):\n",
    "        #Class constructor\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.y_values = y_values\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "            Parameters\n",
    "            ----------\n",
    "            X : {array-like}, shape = [n_samples, n_features], Training vectors, where n_samples is the \n",
    "                        number of samples and n_features is the number of features.\n",
    "            y : array-like, shape = [n_samples], Target values.\n",
    "            Returns\n",
    "            -------\n",
    "            self : object\n",
    "        \"\"\"\n",
    "        #X.shape is tuple (20,2), X.shape[1] will return 2\n",
    "        #w_ will be initialized to an array of 1+2=3 zeros, ie: array([0., 0., 0.])\n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        #initialize errors_ to blank list\n",
    "        self.errors_ = []\n",
    "        #for _ in .... is a way to do the looping but without caring about the number in loop\n",
    "        #range function creates a tuple with first value as 0 and n_iter\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            # zip function can be used to join two lists together, element wise.\n",
    "            # So zip of [[0,0], [0,1]] and [0,1] will give [0, 0] and 0 in first loop, and [0, 1]\n",
    "            # and 1 in second loop\n",
    "            for xi, target in zip(X, y):\n",
    "                #eta will be multiplied by error to find the next update\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "            print(\"weights: {}, rate: {}, iteration: {}, errors: {}\".format(self.w_, \\\n",
    "                                                                            self.eta, \\\n",
    "                                                                            self.n_iter, \\\n",
    "                                                                            self.errors_))\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(\n",
    "            self.net_input(X) >= 0.0, self.y_values[1], self.y_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AND gate using perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_ppt = Perceptron(eta=1, n_iter=10, y_values=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_ppt = and_ppt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(and_ppt.predict([0,0]))\n",
    "print(and_ppt.predict([0,1]))\n",
    "print(and_ppt.predict([1,0]))\n",
    "print(and_ppt.predict([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,1])\n",
    "\n",
    "or_ppt = Perceptron(eta=1,n_iter=10)\n",
    "or_ppt = or_ppt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(or_ppt.predict([0,0]))\n",
    "print(or_ppt.predict([0,1]))\n",
    "print(or_ppt.predict([1,0]))\n",
    "print(or_ppt.predict([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create XOR gate using above perceptron, will fail\n",
    "XOR gate is special in sense its not a linear function and thus the perceptron will fail to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0])\n",
    "\n",
    "xor_ppt = Perceptron(eta=-0.5,n_iter=20)\n",
    "xor_ppt = xor_ppt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xor_ppt.predict([0,0]))\n",
    "print(xor_ppt.predict([0,1]))\n",
    "print(xor_ppt.predict([1,0]))\n",
    "print(xor_ppt.predict([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR solution using DNN and new activation functions\n",
    "XOR cannot be solved using a simple one layer perceptron. Reason for this is XOR is not a **Linearly Separable** function. It needs one input layer, one hidden layer and one output layer.<br>\n",
    "Tanh is better than sigmoid.\n",
    "![NueralNetActivationFunctions](./img/nnet-error-functions.png)\n",
    "For more information please see this\n",
    "__[link](https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activation=\"tanh\", epochs=1000, learning_rate=0.2):\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = tanh\n",
    "            self.activation_derivative = tanh_derivative\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2 * np.random.random((layers[i - 1] + 1, layers[i] + 1)) - 1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2 * np.random.random((layers[i] + 1, layers[i + 1])) - 1\n",
    "        self.weights.append(r)\n",
    "        self.epochs=epochs\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer.\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "\n",
    "        for k in range(self.epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                dot_value = np.dot(a[l], self.weights[l])\n",
    "                activation = self.activation(dot_value)\n",
    "                a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_derivative(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer\n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T) *\n",
    "                              self.activation_derivative(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation\n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += self.learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % (self.epochs/10) == 0: print('epochs:{0}'.format(k))\n",
    "\n",
    "    def predict(self, x):\n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)), axis=0)\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return np.where(abs(a) >= 0.5, 1, 0)\n",
    "        #return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function also fails to converge for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2, 2, 1],\n",
    "                   activation=\"sigmoid\",\n",
    "                   epochs=10000,\n",
    "                   learning_rate=0.2)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "nn.fit(X, y)\n",
    "for e in X:\n",
    "    print(e, nn.predict(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh successfully converges for XOR function even in 10000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2, 2, 1],\n",
    "                   activation=\"tanh\",\n",
    "                   epochs=10000,\n",
    "                   learning_rate=0.2)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "nn.fit(X, y)\n",
    "for e in X:\n",
    "    print(e, nn.predict(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)\n",
    "X = df.iloc[0:100, [0, 2]].values\n",
    "plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='x', label='versicolor')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn = Perceptron(eta=0.1, n_iter=10,y_values=(-1,1))\n",
    "ppn.fit(X, y)\n",
    "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                         np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=cmap(idx),\n",
    "                    marker=markers[idx], label=cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=ppn)\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data sets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(IRIS_TRAINING):\n",
    "        raw = urllib.request.urlopen(IRIS_TRAINING_URL).read()\n",
    "        with open(IRIS_TRAINING, \"wb\") as f:\n",
    "            f.write(raw)\n",
    "\n",
    "    if not os.path.exists(IRIS_TEST):\n",
    "        raw = urllib.request.urlopen(IRIS_TEST_URL).read()\n",
    "        with open(IRIS_TEST, \"wb\") as f:\n",
    "            f.write(raw)\n",
    "# Load datasets.\n",
    "    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "                                                                       filename=IRIS_TRAINING,\n",
    "                                                                       target_dtype=np.int,\n",
    "                                                                       features_dtype=np.float32)\n",
    "    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "                                                                   filename=IRIS_TEST,\n",
    "                                                                   target_dtype=np.int,\n",
    "                                                                   features_dtype=np.float32)\n",
    "# Specify that all features have real-value data\n",
    "    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\n",
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                            hidden_units=[10, 20, 10],\n",
    "                                            n_classes=3,\n",
    "                                            model_dir=\"/tmp/iris_model\")\n",
    "# Define the training inputs\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                                                        x={\"x\": np.array(training_set.data)},\n",
    "                                                        y=np.array(training_set.target),\n",
    "                                                        num_epochs=None,\n",
    "                                                        shuffle=True)\n",
    "# Train model.\n",
    "    classifier.train(input_fn=train_input_fn, steps=2000)\n",
    "# Define the test inputs\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                                                       x={\"x\": np.array(test_set.data)},\n",
    "                                                       y=np.array(test_set.target),\n",
    "                                                       num_epochs=1,\n",
    "                                                       shuffle=False)\n",
    "# Evaluate accuracy.\n",
    "    accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "    print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n",
    "# Classify two new flower samples.\n",
    "    new_samples = np.array(\n",
    "                           [[6.4, 3.2, 4.5, 1.5],\n",
    "                            [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                                                          x={\"x\": new_samples},\n",
    "                                                          num_epochs=1,\n",
    "                                                          shuffle=False)\n",
    "    predictions = list(classifier.predict(input_fn=predict_input_fn))\n",
    "    predicted_classes = [p[\"classes\"] for p in predictions]\n",
    "    print(\"New Samples, Class Predictions:  {}\\n\".format(predicted_classes))\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape = {0}\".format(x_train.shape))\n",
    "print(\"y_train shape = {0}\".format(y_train.shape))\n",
    "print(\"x_test shape = {0}\".format(x_test.shape))\n",
    "print(\"y_test shape = {0}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us graph one of the images from MNIST dataset\n",
    "offset = 0\n",
    "X1 = x_train[offset].reshape([28,28])\n",
    "plt.gray()\n",
    "plt.title(\"Image at offset: {0}\".format(offset))\n",
    "plt.imshow(X1)\n",
    "plt.show()\n",
    "print(\"Label at offset {0} is: {1}\".format(offset, y_train[offset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = [vectorized_result(y) for y in y_train]\n",
    "training_inputs = [np.reshape(x, (784, 1)) for x in x_train]\n",
    "training_data = zip(training_inputs, training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = [vectorized_result(y) for y in y_test]\n",
    "test_inputs = [np.reshape(x, (784, 1)) for x in x_test]\n",
    "test_data = zip(test_inputs, test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "            gradient descent.  The ``training_data`` is a list of tuples\n",
    "            ``(x, y)`` representing the training inputs and the desired\n",
    "            outputs.  The other non-optional parameters are\n",
    "            self-explanatory.  If ``test_data`` is provided then the\n",
    "            network will be evaluated against the test data after each\n",
    "            epoch, and partial progress printed out.  This is useful for\n",
    "            tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                            training_data[k:k+mini_batch_size]\n",
    "                            for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print (\"Epoch {0}: {1} / {2}\".format(\n",
    "                                                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "            gradient descent using backpropagation to a single mini batch.\n",
    "            The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "            is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "            gradient for the cost function C_x.  ``nabla_b`` and\n",
    "            ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "            to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "            network outputs the correct result. Note that the neural\n",
    "            network's output is assumed to be the index of whichever\n",
    "            neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "        \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "            \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])\n",
    "\n",
    "net.SGD(train_data, 30, 10, 3.0, test_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
